\chapter{Implémentation et Résultat}
\label{chap:implementation}
L'implémentation du démonstrateur dans le stage utilise une carte électronique ZYBO\cite{zyboweb}
comme la plateforme principale. Cette plateforme nous permet d'implémenter des matériels
dans le FPGA et des logiciels dans le processeur ARM. 

\section{Implémentation Matérielle}

L'architecture matérielle repose sur la plateforme Zynq qui est constitué d'un \emph{Processing System} (PS) 
basé sur le processeur ARM Cortex-A9 double cœur et un \emph{Programmable Logic} (PL) Xilinx dans
un dispositif, avec une fréquence de 100 MHz. Ce dispositif intègre aussi une mémoire DDR3,
un contrôleur Ethernet et d'autres dispositifs périphériques.

Le diagramme de l'architecture matérielle implémentée dans notre système est présenté sur la Figure \ref{fig:hard}.
La communication entre la partie PS et PL utilise le protocole AXI4 à travers d'un port AXI générique
de 32-bits. 
Le processeur devient le Maître du système et l'IP testé (marqué par DUT\footnote{\emph{Device Under Test}} dans la figure)
avec les interfaces comme les Esclaves.

Pour le démonstrateur construit dans le stage, l'interface AXI4-Lite est utilisée.
L'interface AXI4-Lite est un sous-ensemble de protocole AXI4 utilisé pour une communication
plus simple et un contrôle de registre plus petit dans le composant.
Pour la future implémentation plus rapide, l'autre interfaces AXI4 peut être utilisée. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{hardware}
	\caption{La diagramme de l'architecture matérielle}
	\label{fig:hard}
	\vspace{-2mm}
\end{figure}

L'IP généré par AUGH est lié avec les interfaces d'entrées-sorties personnalisées,
\emph{Input AXI} et \emph{Output AXI} à travers des FIFOs. Les FIFOs sont ajoutées avant la génération du bitstream
afin de s'adapter aux différences de protocole et de la largeur des ports entre le bus AXI et le DUT.

Le DUT utilise le protocole de poignée de main sur ses E/S alors que AXI4-Lite met en œuvre le protocole AXI4.
Ce sont deux protocoles différents, la conversion entre eux devrait être nécessaire . Les \emph{Input AXI} et \emph{Output AXI}
facilitent la conversion de protocole qui permettent la partie PS et DUT de se communiquer. 

En plus du protocole, la largeur de données entre le bus AXI et le DUT peuvent être différentes.
La largeur de données du bus AXI est 32-bits et celle du DUT est variable et dépend de l'application
exécuté, montré par N et M dans la Figure \ref{fig:hard}. Une modification des interfaces est nécessaire afin
de surmonter ce problème. Chaque fois qu'un IP est généré par AUGH, les paramètres de l'IP sont
lus et les interfaces sont ajustées avant la synthèse dans Vivado. 

Afin de transférer les données du bus AXI au DUT, un buffer représenté par un tableau de deux dimensions est utilisé dans
\emph{Input AXI} et \emph{Output AXI}. La taille de buffer dépend de la largeur des ports du DUT. 
Les données du bus AXI sont bufferisées et divisées en plusieurs sorties vers le DUT dans \emph{Input AXI}.
Alors que les sorties du DUT sont bufferisées et sont réunies en une sortie vers le bus AXI dans \emph{Output AXI}.
Par exemple, si le DUT a une entrée de 8-bits, chaque mot de 32-bit du bus AXI est converti à 4 mot de 8-bits par 
\emph{Input AXI}. Cette conversion va changer le nombre de colonne du buffer, les données sont donc multipliées
selon la largeur des ports.

Ces interfaces personnalisées sont conçues pour envoyer des sorties consécutives chaque coup d'horloge
afin d'éviter de goulot d'étranglement même si les largeurs de données E/S sont différentes.

Le \emph{Controller} dans la figure est conçu pour recevoir les commandes du PS, les interpréter et les passer au DUT.
Comme la différence de protocole est toujours présent, un algorithme traduisant les commandes est embarqué dedans.
Le \emph{Timer} est conçu pour calculer le nombre de cycle nécessaire pour exécuter une application testée,
arriver à un point de contrôle et extraire le contexte du DUT.

Dans ce stage, Vivado est utilisé pour générer une interface AXI4 pré-construite avant l'ajout de buffer et de l'algorithme
de conversion du protocole.
L'utilisation de Vivado permet la conception de l'architecture matérielle en mode \emph{batch}, c'est-à-dire sans interface
graphique. Ce mode consomme moins de ressource et est accessible depuis la ligne de commande, qui 
support l'automatisme. Dans le Vivado, le DUT et les autres IP sont intégrés et procédés afin de générer un bitstream
depuis les étapes de synthèse et placement-et-routage.


\section{Implémentation Logicielle}

Comme une plateforme basée sur SoC programmable, Zynq est capable de lancer une application \emph{standalone}
ou une application embarquée dans un système d'exploitation (Linux Embarqué). Dans cette implémentation,
Petalinux, une distribution de Linux Embarqué, est installé afin de lancer l'environnement de pilotage.
Cette approche donne des avantages lors de la reconfiguration FPGA ainsi que la communication via
AXI4 car tous les drivers sont déjà prêts dans le Linux Embarqué.

Parallèlement avec la génération de Linux, les drivers additionnels sont développés et ajoutés en complément
avec les drivers natives de Petalinux. Ces drivers gèrent les tâches d'écriture et de lecture des IP dans la partie PL mené
par la partie PS. L'utilisateur peut appeler ces drivers sur la ligne de commande ainsi que les utiliser dans la script
de bash. Ces drivers sont génériques et utilisables pour n'importe quel DUT.


\subsection{Système d'Exploitation Petalinux}

Dans ce stage, Petalinux 2014.4 est utilisé comme le système d'exploitation principal puisqu'il fournit une référence
intégrale de distribution de linux intégré et est dédié aux dispositifs Xilinx.

La génération/compilation du Petalinux est effectué une fois dans le PC serveur tout début avant que le flot d'exécution n'est commencé.
La génération du Petalinux peut se lancer à chaque fois qu'on le souhaite, mais j'ai décidé de ne pas intégrer ce processus dans le flot
car il n'y a pas de modifications lié au système d'exploitation ou aux drivers.
Après la compilation, les fichiers \textbf{BOOT.BIN} et \textbf{image.ub} sont mis dans la carte SD. La carte démarrera
en cherchant ces deux fichiers.

\subsection{Bibliothèques de logiciels}
\label{subsec:embedded}

Cette partie décrit les bibliothèques de logiciels écrit en langage C. Ces bibliothèques sont ajoutées  
pendant la génération du Petalinux en complément à tous les drivers définis dans sa distribution.

\begin{itemize}
	\item\ com\_axi:
	Cette fonction est une version modifiée d'une fonction prête à utiliser du tutorial ZYBO pour écrire et lire un nombre entier
	de linux aux interfaces AXI4-Lite. Il envoi des valeurs de 32-bits au \emph{Controller} ou lit le nombre de cycles du \emph{Timer}.

	\item\ read\_axi:
	Cette fonction lit des données de l'interface \emph{Output AXI}, écrit les données lues dans un fichier et sauvegarde le fichier au stockage en réseau.
	
	\item\ write\_axi:
	Cette fonction cherche le fichier des vecteurs de test au stockage en réseau, lit les données du fichier et écrit à l'interface \emph{Input AXI}.

	\item\ compare\_axi:
	Cette fonction compare les sorties reçues après l'exécution de l'application avec la référence.
	L'avantage de cette fonction par rapport à la fonction de comparison \emph{diff} en Linux est qu'elle peut montrer le résultat et énumérer les sorties qui sont différents.

	\item\ read\_cp:
	Cette fonction lit le contexte récupérer du DUT dans le \emph{Controller}, écrit le contexte dans un fichier
	et sauvegarde le fichier du contexte au stockage en réseau.
	
	\item\ write\_cp:
	Cette fonction cherche le fichier de contexte, lit les données du fichier et écrit au buffer dans le \emph{Controller}.
	Le contexte va être restauré au DUT par le \emph{Controller} pour continuer la suite de l'exécution.
	
	\item\ save\_time:
	Cette fonction lit le nombre de cycles passé pendant l'exécution, le nombre de cycles d'attente à
	l'extraction du contexte et le nombre de cycles d'extraction du \emph{Timer}, écrit les cycles dans un fichier
	et sauvegarde le fichier au stockage en réseau.
	
	\item\ load\_time:
	Cette fonction cherche le fichier des cycles au stockage en réseau, lit les nombres des cycles
	et les écrit au \emph{Timer} afin que le compteur ne recommence pas depuis 0 quand une application
	s'arrête et est reprise.

\end{itemize}

\subsection{Script de Bash}
Dans cette partie, on parle de script écrit pour lancer le flot de validation automatiquement 
en appelant les drivers expliqués dans la Section \ref{subsec:embedded}.
La partie script est constitué de deux parties; qui vont être exécutées sur le serveur et 
sur la plateforme Zybo. Ces exécutions sont indépendantes et peuvent être
lancées parallèlement. Lorsque le \emph{script\_server} est appelé, il prend
l'IP généré par AUGH et il exécute le processus sur le serveur. Le \emph{script\_zybo}
lance l'exécution dans la plateforme.

\begin{itemize}
	\item\ script\_server:
	Après la génération de l'IP par l'outil de synthèse AUGH, la suite est exécutée avec ce script. Le \emph{script\_server}
	cherche les paramètres nécessaires afin de modifier les interfaces personnalisées et lance Vivado pour la génération
	de l'architecture matérielle en mode de \emph{batch}. Le script exécute la synthèse, le placement et le routage du circuit et 
	la génération de bitstream dans Vivado, ensuite il sauvegarde le bitstream dans le stockage en réseau.
	Il met aussi les vecteurs de test, la référence et le fichier de configuration avec un nom défini.

	\item\ script\_zybo:
	Le \emph{script\_zybo} cherche le bitstream dans le stockage en réseau. Il exécute la reconfiguration de FPGA et démarre
	l'application sur FPGA. Comme le Petalinux intègre le driver de reconfiguration \emph{devcfg}, la reconfiguration peut
	être faite par une commande \emph{xdevcfg}. L'environnement de test est lancé avec les vecteurs de test et les sorties
	sont comparées avec la référence. Ce processus est exécuté dans un ordre séquentiel et il est répétable pour des
	bitstreams et des vecteurs de test différents.
	
\end{itemize}

\subsection{\emph{Network File System}}

Il est nécessaire d'utiliser un protocole de réseau qui permet de faciliter les transferts de fichier entre les utilisateurs,
le serveur et la plateforme ZYBO. Après  avoir considéré le niveau de complexité et la performance, le \emph{Network File System (NFS)}
est choisi pour effectuer le stockage au réseau.

L'implémentation du NFS a besoin deux paquetages: \emph{nfs-kernel-server} au côté du serveur et \emph{nfs-common} au côté du client.
Ce paquetage n'est pas nécéssaire pour la plateforme ZYBO car le Petalinux intègre le client NFS dans son noyau.
En autorisant \emph{portmap} dans la configuration permet le montage de disque NFS au Zynq. Après les paquetages sont installés,
la configuration de disque NFS peut se faire dans le fichier \emph{/etc/exports}.

Le NFS comme le stockage de réseau du système conserve les fichiers suivants :
\begin{itemize}
	\item\ le bitstream à configurer au FPGA,
	\item\ les contextes enregistrés,
	\item\ les nombres de cycles pour chaque tâche exécutée,
	\item\ les jeux de vecteur d'entrées-sorties pour chaque tâche,
	\item\ les sorties de chaque tâche et le résultat d'évaluation.
\end{itemize}

\section{Mesures et Résultat}
\subsection{Validation de la commutation de contexte pour un IP simple}
Un IP simple qui effectue un calcul multiplication et addition itérativement a été crée afin de valider la fonctionnalité de 
l'architecture proposée dans le stage.

\subsection{Validation de la commutation de contexte pour IDCT 2-D}

L'application IDCT\footnote{IDCT : Inverse Discrete Cosine Transform} 2-D est un sous-ensemble de «suite de test» CHStone
qui est souvent utilisé pour valider l'outil de synthèse logique.


\section{Analyse et Discussion}

